## ‚ú® Phase 1: Preparation ‚ú®

Before we dive into Google Cloud, let's get one crucial piece of information! üïµÔ∏è‚Äç‚ôÄÔ∏è

### 1. Find Your Public Home IP Address üè†

We need to tell Google exactly who's allowed to join the party! üéâ

* Open a new browser tab and go to the following website to know your ip:** [https://www.whatismyip.com//](https://www.whatismyip.com/) 
* Google will magically reveal your public IPv4 address (e.g., 136.24.5.112).
* **Write this down!** We'll call it `[YOUR_HOME_IP]`. üìù

---

## üò• The Dreaded "Quota Exceeded" Error! üò•

Don't worry, this is a super common hurdle when starting with GPUs on Google Cloud! It's like a little speed bump on our journey. üöó

### The Problem: The GPU Quota üö´

If you see an error like: `Quota 'GPUS_ALL_REGIONS' exceeded. Limit: 0.0 globally.` it just means your project has a default limit of **zero** GPUs. It's a safety measure to prevent accidental spending. Think of it as a spending cap on your account! üí∞

### The Fix: Request a Quota Increase! üöÄ

We just need to ask Google nicely to increase our GPU quota. It's usually approved super fast! ‚ö°Ô∏è

1. **Go to the Google Cloud Console:** [https://console.cloud.google.com/](https://console.cloud.google.com/)
2. **Search for "Quotas":** In the search bar, type "Quotas" and click on "IAM & Admin > Quotas".
3. **Filter for GPUs:** In the Filter box, type "GPU" and press Enter.
4. **Find the Right Quota:** Look for "Committed NVIDIA L4 GPUs, Region: us-west4". It should show a limit of 0.
5. **Select and Edit:** Click the checkbox next to "GPUs (all regions)" and then the "EDIT QUOTAS" button at the top.
6. **Request Your Quota:**
* **New limit:** Enter `1` (we only need one GPU for now!).
* **Request description:** Write a short, honest reason like: "Running a small AI model (Llama 3) on an L4 GPU for a personal education project on Streamlit."


7. **Submit!** Click "NEXT" and then "SUBMIT REQUEST".

### What Happens Next? ü§î

* You'll get an email confirming your request.
* You'll get another email when it's approved! üéâ
* Once approved, run that `gcloud compute instances create...` command again. It should work like a charm! ‚ú®

---

## üñ•Ô∏è Phase 2: VM Deployment via GUI üñ•Ô∏è

Let's get this VM up and running using the graphical interface! üñ±Ô∏è

* **Name:** `oscp-for-ai`
* **Region:** Select a major region like `us-central1 (Iowa)`.
* **Zone:** Pick any zone, like `us-central1-a`. (If it fails later due to stock, we can always change this!).

### Step 1: Select the Machine and GPU ü§ñ

* Scroll down to **Machine configuration**.
* Click on the **GPUs** tab.
* **GPU type:** Select `NVIDIA L4`.
* **Number of GPUs:** `1`.
* **Machine type:** It should automatically select `g2-standard-4`. If not, select it manually.

### Step 4: Select the Deep Learning Operating System (Crucial Step!) üêß

* Scroll down to the **Boot disk** section.
* Click the **CHANGE** button.
* A new window will pop up. At the top, click on the **PUBLIC IMAGES** tab.
* **Operating system:** Scroll down and select `Deep Learning on Linux`.
* **Version:** Select `Deep Learning VM with CUDA 12.4 M124... (Debian 11)`. (Don't worry if the CUDA version is slightly different, just pick the newest Debian 11 with CUDA!).
* **Boot disk type:** Select `Balanced persistent disk`.
* **Size (GB):** Change the `50` to `100`.
* Click the blue **SELECT** button at the bottom.
---

## üíª Phase 3: VM Deployment via CLI üíª

If you prefer the command line, here's the command to get your VM up and running! üöÄ

```bash
gcloud compute instances create oscp-for-ai \
    --project=coral-mission-481512-i4 \
    --zone=us-central1-a \
    --machine-type=g2-standard-4 \
    --accelerator=count=1,type=nvidia-l4 \
    --image=projects/ml-images/global/images/c0-deeplearning-common-cu124-v20250325-debian-11-py310-conda \
    --boot-disk-size=100GB \
    --boot-disk-type=pd-balanced \
    --tags=streamlit-server,http-server,https-server \
    --network-interface=network-tier=PREMIUM,stack-type=IPV4_ONLY,subnet=default \
    --maintenance-policy=TERMINATE \
    --provisioning-model=STANDARD \
    --restart-on-failure

```


